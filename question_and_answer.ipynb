{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q & A\n",
    "\n",
    "## Questions\n",
    "\n",
    "1. As mentioned in the first section of the paper, just curious what causes spatial transforms to apply \"attention\" to relevant sections of an image? Is it one of: \"the localisation network, grid generator, and sampler\" or the combination of all 3 of them that leads to attentiveness?\n",
    "\n",
    "2. Why is Equation (2) of the paper the \"attention\" transform? Equation (2) being the matrix:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "s & 0 & t[x] \\\\\n",
    "0 & s & t[y] \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "3. Following up on Section 3.4, how/why do spatial transformers minimize the overall cost function of their parent CNN during training?\n",
    "\n",
    "4. Continuing with Section 3.4 again, why do spatial transformers limit the number of objects that can be modeled by a feed-forward network?\n",
    "\n",
    "5. The conclusion section says spatial transforms learn without making changes to their parent CNN's cost function. But Section 3.4 (please see Question #3 above) seems to suggest that spatial transformers do indeed make changes to their parent CNN's cost function. So I'm confused. Please advise if I'm mixing up concepts here and may be the authors are meaning something else in Section 3.4's cost minimization discussion, and something entirely different in the Conclusion section.\n",
    "\n",
    "6. This seems really useful and cool. Do people know what orientation the object is? How do we know that we have to put it face up or face down? \n",
    "\n",
    "\n",
    "7. Can this be extended beyond just normalizing data to creating new data? Or to learn like a manifold?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses\n",
    "\n",
    "##### 1. As mentioned in the first section of the paper, just curious what causes spatial transforms to apply \"attention\" to relevant sections of an image? Is it one of: \"the localisation network, grid generator, and sampler\" or the combination of all 3 of them that leads to attentiveness?\n",
    "\n",
    "it is of course, a combination of all 3, but let's consider them in the order of backpropagation.\n",
    "\n",
    "1. sampler\n",
    "\n",
    "We have your image U and your spatially transformed image V.  Interpolation will determine what sets of points $u_{ij}: {i,j}\\in HW$ \n",
    "the gradients which pass through, and what contributions they have.\n",
    "\n",
    "2. grid generator\n",
    "\n",
    "This piece is really the definition of a spatial transformation. It is how $\\theta$ relates U and V in terms of mapping a point $v_{ij}$ to a position.  Then the sampler determines which $u_{ij}$ to associate for that position.  Now, that means that the grid generator is really what ties $\\theta$ to $U$ and $V$\n",
    "\n",
    "3. localizer network\n",
    "\n",
    "Gradients of $\\theta$ will propogate through the localization network, allowing it to tune the parameters $\\theta$\n",
    "\n",
    "\n",
    "##### 2. Why is Equation (2) of the paper the \"attention\" transform? Equation (2) being the matrix:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "s & 0 & t[x] \\\\\n",
    "0 & s & t[y] \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "square2 = np.zeros((N, N))\n",
    "square2[60:100, 60:100] = 1\n",
    "horse = rescale(skimage.data.horse(), .5, anti_aliasing=True, multichannel=False, mode='constant')\n",
    "coffee = rescale(skimage.data.coffee(), 1.0, anti_aliasing=True, multichannel=True, mode='constant')\n",
    "interp_dict = dict(\n",
    "    [(y, x) for x, y in enumerate(['Nearest Neighbor', 'Linear', 'Quadratic', 'Cubic'])]\n",
    ")\n",
    "def f(tx, ty, s,):\n",
    "    fig=plt.figure(figsize=(12, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    θ =  θ / 180 * np.pi\n",
    "    mat = np.array([\n",
    "        [s * np.cos(θ),  -s * np.sin(θ), tx],\n",
    "        [s * np.sin(θ), s * np.cos(θ), ty],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    xdim = image.shape[1]\n",
    "    ydim = image.shape[0]\n",
    "    shiftR = np.array([\n",
    "            [1, 0, -xdim],\n",
    "            [0, 1, -ydim],\n",
    "            [0, 0, 1] # rigid body\n",
    "        ])\n",
    "    shiftL = np.array([\n",
    "            [1, 0, xdim/2],\n",
    "            [0, 1, ydim/2],\n",
    "            [0, 0, 1] # rigid body\n",
    "        ])\n",
    "    mat = shiftL @ mat @ shiftR\n",
    "    \n",
    "    img = warp(image, mat, output_shape=([2*x for x in image.shape]), \n",
    "               order=interp, mode='constant')\n",
    "    if cmap != 'RGB':\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    plt.grid()\n",
    "\n",
    "def reset_values(b):\n",
    "    for child in plot2.children:\n",
    "        if not hasattr(child, 'description'):\n",
    "            continue\n",
    "        elif child.description in ['tx', 'ty', 'θ', 'kx', 'ky']:\n",
    "            child.value = 0\n",
    "        elif child.description in ['s']:\n",
    "            child.value = 1.0\n",
    "\n",
    "reset_button = widgets.Button(description = \"Reset\")\n",
    "reset_button.on_click(reset_values)\n",
    "\n",
    "x2 = widgets.IntSlider(min=-200, max=200, step=9.8, orientation='vertical', description='$t_x$')\n",
    "y2 = widgets.IntSlider(min=-200, max=200, step=9.8, orientation='vertical', description='$t_y$')\n",
    "s = widgets.FloatSlider(min=0, max=2.0, value=1, orientation='vertical', description='$s$')\n",
    "plot = interactive(f, tx=x2, ty=y2, s=s,interp=interpolation, image=images)\n",
    "layout = Layout(display='flex', flex_flow='row', justify_content='space-between')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
