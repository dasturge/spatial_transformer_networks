{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-766302b79883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msonnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/sonnet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0m_ensure_dependency_available_at_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1.8.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0m_ensure_dependency_available_at_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow_probability'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0.4.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Check some version of TF is available.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/sonnet/__init__.py\u001b[0m in \u001b[0;36m_ensure_dependency_available_at_version\u001b[0;34m(package_name, min_version)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mpip_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpackage_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# pylint: enable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0medward2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=unused-import,wildcard-import,line-too-long,g-importing-member\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_value\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbsoluteValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAffine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_linear_operator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAffineLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/absolute_value.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/bijector.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribution_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/internal/distribution_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtype_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreparameterization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmart_cond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_probability/python/internal/dtype_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontrib_framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m'tf.contrib.learn.basic_session_run_hooks.StopAtStepHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m'tf.train.StopAtStepHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     basic_session_run_hooks.StopAtStepHook)\n\u001b[0m\u001b[1;32m     38\u001b[0m CheckpointSaverHook = deprecated_alias(\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m'tf.contrib.learn.basic_session_run_hooks.CheckpointSaverHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mdeprecated_alias\u001b[0;34m(deprecated_name, name, func_or_class, warn_once)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Make a new class with __init__ wrapped in a warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mclass\u001b[0m \u001b[0m_NewClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m       __doc__ = decorator_utils.add_notice_to_docstring(\n\u001b[1;32m    175\u001b[0m           \u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please use %s instead.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36m_NewClass\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m                            'It will be removed in a future version. '])\n\u001b[1;32m    179\u001b[0m       \u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0m__module__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;34m@\u001b[0m\u001b[0m_wrap_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36m_call_location\u001b[0;34m(outer)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;34m\"\"\"Returns call location given level up from current call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# CPython internals are available, use them for performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1450\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import skimage\n",
    "import skimage.data\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keras = tf.keras\n",
    "K = keras.backend\n",
    "\n",
    "from ipywidgets import interact, interactive, Layout, HBox\n",
    "from skimage.transform import rescale, warp, ProjectiveTransform\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# extra code\n",
    "import helpers\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Transformer Networks\n",
    "\n",
    "[Spatial Transformer Networks, by Jaderberg, M. et. al](https://arxiv.org/pdf/1506.02025.pdf) explores a novel convolutional neural network (CNN) module designed by the authors at Google Deepmind.\n",
    "Spatial Transformers, aptly named, allow a network to incorporate spatial transformations, typically on the input image, which can have the effect of cropping, reorienting, or undistorting the image in question.\n",
    "The authors show how spatial transformations can take the place of attention networks, and additionally solve previous problems with translational invariance in CNNs.\n",
    "They can be added seamlessly to any neural network.\n",
    "They are differentiable, and therefore can be trained through simple backpropogation.\n",
    "\n",
    "First, let's consider the motivations behind spatial transformer networks, starting with attention mechanisms in neural architecture.\n",
    "\n",
    "### Attention\n",
    "\n",
    "Attention mechanisms in neural networks give them the ability to focus on subsets of features. In image processing, this takes the shape of spatial localization of the features. In the paper Show,\n",
    "\n",
    "![Image](img/show-attend-tell.png)\n",
    "\n",
    "[image from Show, Attend, and Tell](https://arxiv.org/abs/1502.03044)\n",
    "\n",
    "Why is this so effective?  Here's the intuition I could come up with: Part of it is the problem space of most testing with CNNs: Single object classification or some single object-based task. Even extending this to several objects, there is often a large amount of irrelevant background information in the images. As such, this is an effective dimensionality reduction technique. \n",
    "\n",
    "To contrast, it may not be effective for segmentation of a large number of objects, or for semantic segmentation tasks, or for something with extracting texture features.\n",
    "In other words, tasks where you might expect important features to be comprehensively spread through the image.\n",
    "\n",
    "In CNNs, we can consider attention in other image processing terms, taking the form of a binary or probability mask over an image (or layer), thereby focusing the effective activations within a small region.\n",
    "This naive method of attention works, but with a heavy computational load. Kernels must still be computed over the whole image.\n",
    "\n",
    "Another proposed option, is to crop the image.\n",
    "However, this method also faces problems, as it so far has been non-differentiable, so some sort of stochastic or baked-in cropping method must be implemented.\n",
    "\n",
    "As we'll see, image cropping can actually be modeled by a constrained linear transformation.\n",
    "\n",
    "\n",
    "### Translational Invariance\n",
    "\n",
    "$\\forall \\delta \\ A f = A T_\\delta f$\n",
    "\n",
    "**Translational invariance** is the property that transforming a feature map does not affect the resulting output. Convolutional Neural networks do not have this feature, instead, because of parameter sharing, they have the property that they are **translationally equivariant**:\n",
    "\n",
    "$\\forall \\delta \\  T_\\delta A f = A T_\\delta f$\n",
    "\n",
    "this *would* be totally sufficient, except that eventually, we are going to feed the network into a dense layer.  Dense layers by definition have no spatial relationships built in, so we need the CNN to be able to feed some proper positional information to the dense layer.\n",
    "\n",
    "In a sense, max pooling layers create some \"translational invariance\" by only accepting one input of a local region.  Of course, this is only a very slight translation for each layer, so it's not nearly the property we're looking for.\n",
    "\n",
    "\n",
    "Next, let's quickly go over basic spatial transforms.\n",
    "\n",
    "The two main concepts involved here are spatial transformations, and sampling:\n",
    "\n",
    "### Spatial Transformations\n",
    "\n",
    "For a much deeper look at these spatial transformations, [check out this resource](https://people.cs.clemson.edu/~dhouse/courses/401/notes/affines-matrices.pdf)\n",
    "\n",
    "A \"spatial transformation\" is a transformation on the *coordinate system* of an image.\n",
    "\n",
    "Let a pixel $P$ be defined at indices: $(n, m) \\in \\mathbb{N}$ \n",
    "\n",
    "then the transformation $T_\\theta$ maps the pixel $P$ to a new point $(x', y') \\in \\mathbb{R}^2$\n",
    "\n",
    "The most basic forms of spatial transformations are linear or projective transformations, which can be represented with matrix multiplication.\n",
    "Let's look at some examples of these matrices, to have a look at what the *parameters* of such transformations are.\n",
    "\n",
    "##### linear\n",
    "\n",
    "Suppose we consider each pair of integer indices in the image, $U$, to be a set of spatial coordinates. Let's denote the indices as $n$, and $m$.  The spatial transformation will map each pair of indices $(n, m)$ to a pair $(x', y')$.\n",
    "\n",
    "For linear transformations, we can easily represent this transformation as a matrix. Then, depending on *how* we want to transform the matrix, we can \n",
    "\n",
    "for a \"rigid body\" transformation, we have 3 parameters, $t_x, t_y, \\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) & -t_x \\\\ \\sin(\\theta) & \\cos(\\theta) & -t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} n \\\\ m \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "a \"similarity\" transformation, with 4 parameters $s, t_x, t_y, \\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} s \\cos(\\theta) &  -s \\sin(\\theta) & -t_x \\\\ s \\sin(\\theta) & s \\cos(\\theta) & -t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} n \\\\ m \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for more general, \"affine\" transformations, 6 parameters $a, b, c, d, t_x, t_y, \\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} n \\\\ m \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "##### nonlinear\n",
    "\n",
    "finally, \"homographic\" or \"projective\" transformations (our first nonlinear transform), with 8 parameters:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ e & f & t_z \\end{bmatrix} \\begin{bmatrix} n \\\\ m \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These parameters are also called the \"degrees of freedom\" of the transform, where the degrees of freedom are the minimum number of parameters required to fully specify that type of transformation.\n",
    "In the projective transform, although the number of variables shown is 9, the last row has the constraint of always summing to 1, hence you can remove one parameter.\n",
    "\n",
    "### sampling\n",
    "\n",
    "the caveat with these transforms, is that $(x', y')$ are typically not integers, which conflicts with the *digital* representation of images.\n",
    "We will need to now find the values of the transformed image at new integer points $(n', m') \\in \\mathbb{N}$\n",
    "\n",
    "To handle this, we'll need sampling theory.\n",
    "\n",
    "**Interactive Notebook Kernel**: if this notebook is being run on a kernel, we can explore affine transforms and sampling with the interactive widget below. We shall also discuss sampling more later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "square2 = np.zeros((N, N))\n",
    "square2[60:100, 60:100] = 1\n",
    "horse = rescale(skimage.data.horse(), .5, anti_aliasing=True, multichannel=False, mode='constant')\n",
    "coffee = rescale(skimage.data.coffee(), 1.0, anti_aliasing=True, multichannel=True, mode='constant')\n",
    "interp_dict = dict(\n",
    "    [(y, x) for x, y in enumerate(['Nearest Neighbor', 'Linear', 'Quadratic', 'Cubic'])]\n",
    ")\n",
    "def f(tx, ty, θ, s, kx, ky, interp=None, image='square'):\n",
    "    fig=plt.figure(figsize=(8, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    interp = interp_dict[interp]  # skimage uses integer codes for interpolation.\n",
    "    if interp == 2:\n",
    "        print(\"ERROR: can't use thin plate spline interpolation due to a bug in scipy\")\n",
    "        interp = 1\n",
    "    θ =  θ / 180 * np.pi\n",
    "    mat = np.array([\n",
    "        [s * np.cos(θ),  -s * (np.sin(θ) + kx), tx],\n",
    "        [s * (np.sin(θ) + ky), s * np.cos(θ), ty],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    if image == 'square':\n",
    "        image = square2\n",
    "        cmap = 'gray'\n",
    "    elif image == 'horse':\n",
    "        image = horse\n",
    "        dims = image.shape\n",
    "        cmap = 'gray'\n",
    "    elif image == 'coffee':\n",
    "        image = coffee\n",
    "        cmap = 'RGB'\n",
    "    xdim = image.shape[1]\n",
    "    ydim = image.shape[0]\n",
    "    shiftR = np.array([\n",
    "            [1, 0, -xdim],\n",
    "            [0, 1, -ydim],\n",
    "            [0, 0, 1] # rigid body\n",
    "        ])\n",
    "    shiftL = np.array([\n",
    "            [1, 0, xdim/2],\n",
    "            [0, 1, ydim/2],\n",
    "            [0, 0, 1] # rigid body\n",
    "        ])\n",
    "    mat = shiftL @ mat @ shiftR\n",
    "    \n",
    "    img = warp(image, mat, output_shape=([2*x for x in image.shape]), \n",
    "               order=interp, mode='constant')\n",
    "    if cmap != 'RGB':\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    plt.grid()\n",
    "\n",
    "def reset_values(b):\n",
    "    for child in plot2.children:\n",
    "        if not hasattr(child, 'description'):\n",
    "            continue\n",
    "        elif child.description in ['tx', 'ty', 'θ', 'kx', 'ky']:\n",
    "            child.value = 0\n",
    "        elif child.description in ['s']:\n",
    "            child.value = 1.0\n",
    "\n",
    "reset_button = widgets.Button(description = \"Reset\")\n",
    "reset_button.on_click(reset_values)\n",
    "\n",
    "x2 = widgets.IntSlider(min=-200, max=200, step=9.8, orientation='vertical', description='$t_x$')\n",
    "y2 = widgets.IntSlider(min=-200, max=200, step=9.8, orientation='vertical', description='$t_y$')\n",
    "t2 = widgets.IntSlider(min=-180, max=180, step=18, orientation='vertical', description=r'$\\theta$')\n",
    "kx = widgets.FloatSlider(min=-2.0, max=2.0, value=0.0, orientation='vertical', description='$k_x$')\n",
    "ky = widgets.FloatSlider(min=-2.0, max=2.0, value=0.0, orientation='vertical', description='$k_y$')\n",
    "s = widgets.FloatSlider(min=0, max=2.0, value=1, orientation='vertical', description='$s$')\n",
    "interpolation = widgets.RadioButtons(\n",
    "    options=['Nearest Neighbor', 'Linear', 'Quadratic', 'Cubic'][::-1], value='Linear', description='interpolation')\n",
    "images = widgets.RadioButtons(options=['square', 'horse', 'coffee'])\n",
    "plot2 = interactive(f, tx=x2, ty=y2, θ=t2, s=s, kx=kx, ky=ky,interp=interpolation, image=images)\n",
    "layout = Layout(display='flex', flex_flow='row', justify_content='space-between')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot2.update()\n",
    "display(HBox([plot2.children[-1], images]))\n",
    "display(HBox([*plot2.children[:-2], reset_button], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widget above shows for a particular parameterization (defined on the sliders) of the affine transformation, what the end effect will be on the image.\n",
    "You can choose between the square, an outline of a horse, and a photograph of a coffee cup.\n",
    "I encourage you to test out both the available parameters.\n",
    "You can also toggle different interpolation methods, to see how choice of sampling affects the output image (quadratic does not work due to a bug in the current version of scikit-image).\n",
    "\n",
    "### The Spatial Transformer Module: 3 pieces\n",
    "\n",
    "The Spatial Transformer consists of three pieces, a \"localization network\", a \"grid generator\", and a sampler\\*.\n",
    "\n",
    "![network configuration](img/STN_figure_2.png)\n",
    "\n",
    "##### the localization network\n",
    "\n",
    "The localization network is how the spatial transformation is decided for each input in the forward pass. \n",
    "It's simple in concept, it is a standard CNN, where the output units are the free parameters in the transformation model we choose (e.g. affine, similarity, projective, etc.).\n",
    "For example, if we wanted an affine transformation, the localization network should have 6 output units.  For a similarity transform, 4 output units, etc.\n",
    "The rest is taken care of by backpropogation!\n",
    "The weight initialization should be small, making the model very close to an identity transform at first.\n",
    "\n",
    "##### the grid generator\n",
    "\n",
    "The transformation T is generated from the above network's parameters. Then we need to compute *which* points on the image we are sampling from after the transformation is applied, then place them into an output layer. The output layer may need not strictly be the same size as the input. In fact, the size of the output layer can be freely chosen.\n",
    "\n",
    "![grid generator](img/STN_figure_3.png)\n",
    "\n",
    "\n",
    "In the image above we can see the identity transformation contrasted with an affine transformation. The sampling grid generates the points which must be sampled onto. This is illustrated in the example above, where we see the quadrilateral of points on $U$ going directly into the next layer $V$. However, just like in the initial discussion of transformations, the points on the quadrilateral will not, in general, correspond directly to pixel coordinates in $U$. This is why we will need to *resample* the points.\n",
    "\n",
    "##### sampling\n",
    "\n",
    "The remaining issue is to recompute the value of the points we sample from on the image.  Because they are in between pixel coordinates, they require sampling. *In this context, sampling has nothing to do with statistical sampling, it is interpolation.* The widget above does a good job of giving a visual inspection of sampling, let's look at how this occurs in the backpropogation.\n",
    "\n",
    "\n",
    "### backpropagation\n",
    "\n",
    "Take equation **(3)** from the paper, which gives the tranformed image $V$ from the input image $U$.\n",
    "\n",
    "$$\n",
    "V_i^c = \\sum_n^H \\sum_m^W U_{nm}^c k(x_i^s - m; \\Phi_x) k(y_i^s - n; \\Phi_y) \\forall i \\in [1...H'W'] \\forall c \\in [1...C]\n",
    "$$\n",
    "\n",
    "$k$: the sampling kernel  \n",
    "$\\Phi$: sampling kernel parameters  \n",
    "$H, W, C$: heighth, width, channels of U  \n",
    "$H', W'$: heighth, width of V  \n",
    "$n, m$: indices over heighth and width (pixel coordinates) of U  \n",
    "$i$: flat index over all points in the transformed image, V   \n",
    "$x_i^s, y_i^s$: transformed $x$, $y$ coordinates from $T_\\theta(G)$  \n",
    "$c$: index over channels, if relevant\n",
    "\n",
    "This can seem a little obtuse at first:\n",
    "\n",
    "the key things to note are:\n",
    "\n",
    "- in practice, the sampling kernel $k_\\Phi$ typically behaves as a delta function, eliminating the contribution of *most points* in the image. So this is not actually a massive sum over the image, typically only 1, 4, or 16 coordinates for nearest neighbor, bilinear, and bicubic interpolation respectively.\n",
    "\n",
    "- $x_i^s$, $y_i^s$, correspond to $m$, $n$, except $x_i^s$, $y_i^s$ are continous values where $m$, $n$ are integers.  That is the problem which sampling *addresses*, to generate values for $V$ in the integer domain.\n",
    "\n",
    "- channels of the image are all transformed equivalently, so they shouldn't be given too much thought in the process.\n",
    "\n",
    "\n",
    "Let's look at the bilinear interpolation example, equation **(5)**, to review backpropogation:\n",
    "\n",
    "$$\n",
    "V_i^c = \\sum_n^H \\sum_m^W U_{nm}^c \\max(0, 1 - |x_i^s - m|) \\max(0, 1 - |y_i^s - n|)\n",
    "$$\n",
    "\n",
    "Now, taking the gradient of this function with respect to the input layer $U$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V_i^c}{\\partial U_{mn}^c} = \\sum_n^H \\sum_m^W \\max(0, 1 - |x_i^s - m|) \\max(0, 1 - |y_i^s - n|)\n",
    "$$\n",
    "\n",
    "Then, taking the gradient with respect to $x_i^s$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial V_i^c}{\\partial x_i^s} &= \\frac{\\partial}{\\partial x_i^s} \\sum_n^H \\sum_m^W U_{nm}^c \\max(0, 1 - |x_i^s - m|) \\max(0, 1 - |y_i^s - n|) \\\\\n",
    "&= \\begin{cases} \\sum_n^H \\sum_m^W U_{nm}^c  \\max(0, 1 - |y_i^s - n|) & m - 1 < x_i^s < m \\\\\n",
    "-\\sum_n^H \\sum_m^W U_{nm}^c  \\max(0, 1 - |y_i^s - n|) & m < x_i^s < m + 1 \\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can see here that the kernel heavily constrains the number of points involved in each gradient, so only local affects should play a large role. When $\\frac{\\partial x}{\\partial \\theta}$ is considered, it completes the picture for how spatial transformations relate to gradient flow.\n",
    "\n",
    "\n",
    "## By Example\n",
    "\n",
    "Now let's see some Spatial Transformer Networks in action!\n",
    "\n",
    "We'll start with MNIST, certainly not for showing the effectiveness, but just as a simple proof of concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(y_train[..., np.newaxis])\n",
    "y_test = onehot_encoder.fit_transform(y_test[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions for visualizations\n",
    "def plot_examples(images, cls_true, cls_pred=None):\n",
    "    n_examples = images.shape[0]\n",
    "    m = int(np.ceil(np.sqrt(n_examples)))\n",
    "    fig, axes = plt.subplots(m, m)\n",
    "    fig.set_size_inches((12, 11))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if len(images.shape) == 4:\n",
    "            ax.imshow(images[i, :, :, 0], cmap='gray')\n",
    "        elif len(images.shape) == 3:\n",
    "            ax.imshow(images[i], cmap='gray')\n",
    "        if len(cls_true.shape) > 1:\n",
    "            cls = np.argmax(cls_true[i])\n",
    "        else:\n",
    "            cls = cls_true[i]\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"Label: {}\".format(cls)\n",
    "        else:\n",
    "            if len(cls_pred.shape) > 1:\n",
    "                pred = np.argmax(cls_pred[i])\n",
    "            else:\n",
    "                pred = cls_pred[i]\n",
    "            xlabel = \"True: {}, Pred: {}\".format(cls, pred)\n",
    "        \n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at your data\n",
    "\n",
    "Let's just take a look at some data using our plotting helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(X_train[20:29, :, :, 0], y_train[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### % Work in Progress % Random Preprocessing \n",
    "\n",
    "So this dataset is pretty vanilla.  We don't actually expect spatial transformations to do much, if anything, to digits which are already properly oriented.\n",
    "\n",
    "When I have some extra time I will add some randomization to the mix to test the limitations of the spatial transformer a bit further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "We'll need to set up the pieces of the spatial transformer module, and then plug it into a standard CNN.\n",
    "\n",
    "First, we'll define the **localizer**.\n",
    "Remember that this is really the same as a small CNN. In a naive approach, I'm simply putting some parameters at the end, forced into a **sigmoid activation**, without concern for their meaning within a transformation matrix...\n",
    "\n",
    "I'll also define a **standard cnn** for classification.\n",
    "This will help later for comparing STN results to regular networks, as it will serve as both a stand alone CNN, and as the CNN we feed the output of the Spatial Transformer into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localizer(input_layer, constraints):\n",
    "    if constraints is not None:\n",
    "        num_params = constraints.num_free_params\n",
    "    else:\n",
    "        num_params = 6\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(graph)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(graph)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Flatten()(graph)\n",
    "    theta = keras.layers.Dense(units=num_params, activation='sigmoid')(graph)\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "def cnn_model_fn(input_layer, dense_units, num_classes):\n",
    "    graph = input_layer\n",
    "    for i in range(2):\n",
    "        graph = keras.layers.Conv2D(filters=32 * 2**i, kernel_size=(3, 3), activation='relu')(graph)\n",
    "        graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Flatten()(graph)\n",
    "    graph = keras.layers.Dense(\n",
    "        units=dense_units, activation='sigmoid')(graph)\n",
    "    dense = keras.layers.Dense(num_classes)(graph)\n",
    "    logits = keras.layers.Activation('softmax', name='y')(dense)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def standard_cnn(**pm):\n",
    "    X = keras.layers.Input(\n",
    "        shape=pm['input_shape'], name='X')\n",
    "    output = cnn_model_fn(X, pm['dense_units'], pm['num_classes'])\n",
    "    model = keras.Model(inputs=X, outputs=output)\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a special module, using Deepmind Sonnet's tensorflow package.\n",
    "This module, which I'm calling the Spatial Transform Layer, is meant to take care of generating the grid points and the resampling.\n",
    "\n",
    "I've defined it to accept a tuple of inputs: the input image, and the transform parameters (the output layer of the localization network). The tuple input works more seamlessly with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformLayer(snt.AbstractModule):\n",
    "    \"\"\"\n",
    "    affine transform layer.\n",
    "    Constructor requires output_shape, with optional\n",
    "    constraints and boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_shape,\n",
    "                 constraints=None,\n",
    "                 name='stn_layer'):\n",
    "        \"\"\"\n",
    "        :param output_shape: shape of output layer (not including batch size)\n",
    "        :param constraints: AffineWarpConstraints object from dm-sonnet package\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        if constraints is None:\n",
    "            # default is full affine transform\n",
    "            constraints = snt.AffineWarpConstraints.no_constraints(num_dim=2)\n",
    "        self._constraints = constraints\n",
    "        self._output_shape = output_shape\n",
    "        self.__name__ = name\n",
    "\n",
    "    def _build(self, inputs):\n",
    "        \"\"\"\n",
    "        Layer requires a tuple of arguments: an input layer\n",
    "        and a set of transform parameters.\n",
    "        \"\"\"\n",
    "        U, theta = inputs\n",
    "        grid = snt.AffineGridWarper(\n",
    "            source_shape=U.get_shape().as_list()[1:-1],\n",
    "            output_shape=self._output_shape,\n",
    "            constraints=self._constraints)(theta)\n",
    "        V = tf.contrib.resampler.resampler(U, grid, name='resampler')\n",
    "\n",
    "        return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we have the tools to finalize the model.\n",
    "The format is basic, just to take inputs and transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our keras model\n",
    "def stn_model_fn(**pm):\n",
    "\n",
    "    # input layer\n",
    "    U = keras.layers.Input(\n",
    "        shape=pm['input_shape'], name='X')\n",
    "\n",
    "    # create localizationnetwork\n",
    "    theta = localizer(U, pm['constraints'])\n",
    "\n",
    "    grid_resampler = SpatialTransformLayer(output_shape=pm['output_shape'],\n",
    "                                           constraints=pm['constraints'])\n",
    "\n",
    "    # feed input U and parameters theta into the grid resampler\n",
    "    V = keras.layers.Lambda(\n",
    "        grid_resampler, output_shape=pm['output_shape'], name='V')([U, theta])\n",
    "\n",
    "    # now input V to a standard cnn\n",
    "    logits = cnn_model_fn(V, pm['dense_units'], pm['num_classes'])\n",
    "    \n",
    "    model = keras.Model(inputs=U, outputs=logits)\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# create some boundaries on the affine warp\n",
    "constraints = snt.AffineWarpConstraints([[None, None, None],\n",
    "                                         [None, None, None]])\n",
    "\n",
    "model_parameters = {\n",
    "    'input_shape': (28, 28, 1),\n",
    "    'batch_size': 32,\n",
    "    'output_shape': (28, 28),\n",
    "    'constraints': constraints,\n",
    "    'dense_units': 128,\n",
    "    'num_classes': 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Models\n",
    "\n",
    "Now that we've written up the models themselves, let's initialize each a **Standard CNN** and a **Spatial Transformer Module hooked up to a Standard CNN**\n",
    "\n",
    "Since the CNNs have the same basic architecture, we expect to see how STNs may change what goes in, and hopefully, get a glimpse of how they change kernels of the model later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normal_model = standard_cnn(**model_parameters)\n",
    "normal_model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stn_model_fn(**model_parameters)\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the STN getting wrong?\n",
    "\n",
    "We end up with 95% accuracy in the STN and 98% accuracy in the standard CNN.  So what gives?\n",
    "\n",
    "I should take this time to note, the accuracy of the STN was quite random, indicating a serious problem with initialization, quite frequently it was near chance, 10%.  *Can you guess why?*\n",
    "\n",
    "let's look at some examples where the STN is having trouble classifying, to see what might be going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = keras.Model(inputs=model.input,\n",
    "                        outputs=model.get_layer('V').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some samples\n",
    "y_pred = model.predict(X_test)\n",
    "cls_pred = np.argmax(y_pred, axis=0)\n",
    "cls_true = np.argmax(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_preds = np.argwhere(cls_pred != cls_true).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = bad_preds[:9]\n",
    "U_images = X_test[examples]\n",
    "V_images = resampler.predict(U_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(U_images, cls_true[examples], cls_pred[examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(V_images, cls_true[examples], cls_pred[examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Localizer Network is more nuanced than the paper admits\n",
    "\n",
    "The authors don't make any statements about just how sensitive these layers are. Think about it:  Out of all the possible transformations, there are *a lot of them* that actually land outside of the original image entirely, or grab a useless zero region in the MNIST case. To fix this, we redefine the localizer network. \n",
    "\n",
    "Let's give it strict activation functions such as sigmoid and tanh, then add an identity matrix to the final theta layer.\n",
    "The idea here is that the parameters theta should start very small, and slowly learn how to change from an identity transformation (retaining the image $U$ as-is) to finding a convenient transform to $V$ to assist the standard CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localizer(input_layer, constraints):\n",
    "    if constraints is not None:\n",
    "        num_params = constraints.num_free_params\n",
    "    else:\n",
    "        num_params = 6\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='sigmoid')(input_layer)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='sigmoid')(graph)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='sigmoid')(graph)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Flatten()(graph)\n",
    "    graph = keras.layers.Dense(units=num_params, activation='tanh', bias_initializer='zeros', activity_regularizer='l1')(graph)\n",
    "    # hard code the affine transformation!\n",
    "    theta = keras.layers.Dense(units=num_params, bias_initializer='zeros', activity_regularizer='l1')(graph)\n",
    "    def add(theta):        \n",
    "        identity = tf.constant([[1, 0, 0, 0, 1, 0]], dtype=tf.float32)\n",
    "        theta = theta + identity\n",
    "        return theta\n",
    "    \n",
    "    theta = keras.layers.Lambda(add)(theta)\n",
    "    print(theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stn_model_fn(**model_parameters)\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The performance is now much better, but let's see what this new model is having trouble with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some samples\n",
    "y_pred = model.predict(X_test)\n",
    "cls_pred = np.argmax(y_pred, axis=1)\n",
    "cls_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_preds = np.argwhere(cls_pred != cls_true).flatten()\n",
    "examples = bad_preds[:9]\n",
    "U_images = X_test[examples]\n",
    "V_images = resampler.predict(U_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(U_images, cls_true[examples], cls_pred[examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(V_images, cls_true[examples], cls_pred[examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_images = X_test[:9]\n",
    "V_images = resampler.predict(U_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pred[examples].shape"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "c917848893404fc48728f4d1d4293888",
   "lastKernelId": "3e71fc26-34fe-4fdd-bc53-d32686f52140"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
