{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/euler/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import skimage\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keras = tf.keras\n",
    "K = keras.backend\n",
    "\n",
    "from ipywidgets import interact, interactive, Layout, HBox\n",
    "from skimage.transform import rescale, warp, ProjectiveTransform\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# extra code\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Transformer Networks\n",
    "\n",
    "[Spatial Transformer Networks, by Jaderberg, M. et. al](https://arxiv.org/pdf/1506.02025.pdf) explores a new convolutional neural network (CNN) module designed by the authors at Google Deepmind.\n",
    "It allows a network to incorporate spatial transformations, typically on the input layer, which can have the effect of cropping, reorienting, or undistorting the layer in question.\n",
    "The authors show how spatial transformations can take the place of attention networks, and additionally solve previous problems with translational invariance in CNNs.\n",
    "It can be added seamlessly to any neural network. \n",
    "It is differentiable, and and therefore trained through simple backpropogation.\n",
    "\n",
    "First, let's consider the motivations behind spatial transformer networks, starting with attention mechanisms in neural architecture.\n",
    "\n",
    "### Attention\n",
    "\n",
    "Attention mechanisms in neural networks give them the ability to focus on subsets of features.\n",
    "\n",
    "![Image]()\n",
    "\n",
    "Why is this so effective?  Here's the intuition I could come up with: Part of it is the problem space of most testing with CNNs: Single object classification or some single object-based task. Even extending this to several objects, there is often a large amount of irrelevant background information in the images. As such, this is an effective dimensionality reduction technique. \n",
    "\n",
    "To contrast, it may not be effective for segmentation of a large number of objects, or for semantic segmentation tasks, or for something like predicting if artwork will be popular.\n",
    "In other words, tasks where you might expect important features to be comprehensively spread through the image.\n",
    "\n",
    "In CNNs, we can consider attention in other image processing terms, taking the form of a binary or probability mask over an image (or layer), thereby focusing the effective activations within a small region.\n",
    "This naive method of attention works, but with a heavy computational load. Kernels must still be computed over the whole image.\n",
    "\n",
    "Another proposed option, is to crop the image.\n",
    "However, this method also faces problems, as it so far has been non-differentiable, so some sort of stochastic or baked-in cropping method must be implemented.\n",
    "\n",
    "As we'll see, image cropping can actually be modeled by a constrained linear transformation.\n",
    "\n",
    "\n",
    "### Translational Invariance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Next, let's quickly go over basic spatial transforms.\n",
    "\n",
    "The two important concepts involved here are:\n",
    "\n",
    "### Spatial Transformations\n",
    "\n",
    "For a much deeper look at these spatial transformations, [check out this resource](https://people.cs.clemson.edu/~dhouse/courses/401/notes/affines-matrices.pdf)\n",
    "\n",
    "A \"spatial transformation\" is a transformation on the *coordinate system* of an image.\n",
    "\n",
    "Let a pixel $P$ be defined at: $(x, y)$\n",
    "\n",
    "then the transformation $T_\\theta$ maps the pixel $P$ to a point $(x', y')$\n",
    "\n",
    "The most basic forms of spatial transformations are linear or projective transformations, which can be represented with matrix multiplication.\n",
    "Let's look at some examples of these matrices, to have a look at what the *parameters* of such transformations are.\n",
    "\n",
    "##### linear\n",
    "\n",
    "for a \"rigid body\" transformation, we have 3 parameters, $t_x, t_y, \\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) & -t_x \\\\ \\sin(\\theta) & \\cos(\\theta) & -t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "a \"similarity\" transformation, with 4 parameters $s, t_x, t_y, \\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} s \\cos(\\theta) &  -s \\sin(\\theta) & -t_x \\\\ s \\sin(\\theta) & s \\cos(\\theta) & -t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for more general, \"affine\" transformations, 6 parameters $a, b, c, d, t_x, t_y, \\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "finally, \"homographic\" or \"projective\" transformations (our first nonlinear transform), with 8 parameters:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} a & b & t_x \\\\ c & d & t_y \\\\ e & f & t_z \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These parameters are also called the \"degrees of freedom\" of the transform, where the degrees of freedom are the minimum number of parameters required to fully specify that type of transformation.\n",
    "In the projective transform, although the number of variables shown is 9, the last row has the constraint of always summing to 1, hence you can remove one parameter.\n",
    "\n",
    "### sampling\n",
    "\n",
    "the caveat with these transforms, is that $(x', y')$ are typically not integers, which conflicts with the *digital* representation of images.\n",
    "To handle this, we'll need sampling theory.\n",
    "\n",
    "\n",
    "**Interactive Notebook Kernel**: if this notebook is being run on a kernel, we can explore affine transforms and sampling with the interactive widget:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "square2 = np.zeros((N, N))\n",
    "square2[60:100, 60:100] = 1\n",
    "horse = rescale(skimage.data.horse(), .5, anti_aliasing=True, multichannel=False, mode='constant')\n",
    "coffee = rescale(skimage.data.coffee(), 1.0, anti_aliasing=True, multichannel=True, mode='constant')\n",
    "interp_dict = dict(\n",
    "    [(y, x) for x, y in enumerate(['Nearest Neighbor', 'Linear', 'Quadratic', 'Cubic'])]\n",
    ")\n",
    "def f(tx, ty, θ, s, kx, ky, interp=None, image='square'):\n",
    "    fig=plt.figure(figsize=(12, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    interp = interp_dict[interp]  # skimage uses integer codes for interpolation.\n",
    "    if interp == 2:\n",
    "        print(\"ERROR: can't use thin plate spline interpolation due to a bug in scipy\")\n",
    "        interp = 1\n",
    "    θ =  θ / 180 * np.pi\n",
    "    mat = np.array([\n",
    "        [s * np.cos(θ),  -s * (np.sin(θ) + kx), tx],\n",
    "        [s * (np.sin(θ) + ky), s * np.cos(θ), ty],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    if image == 'square':\n",
    "        image = square2\n",
    "        cmap = 'gray'\n",
    "    elif image == 'horse':\n",
    "        image = horse\n",
    "        dims = image.shape\n",
    "        cmap = 'gray'\n",
    "    elif image == 'coffee':\n",
    "        image = coffee\n",
    "        cmap = 'RGB'\n",
    "    xdim = image.shape[1]\n",
    "    ydim = image.shape[0]\n",
    "    shiftR = np.array([\n",
    "            [1, 0, -xdim],\n",
    "            [0, 1, -ydim],\n",
    "            [0, 0, 1] # rigid body\n",
    "        ])\n",
    "    shiftL = np.array([\n",
    "            [1, 0, xdim/2],\n",
    "            [0, 1, ydim/2],\n",
    "            [0, 0, 1] # rigid body\n",
    "        ])\n",
    "    mat = shiftL @ mat @ shiftR\n",
    "    \n",
    "    img = warp(image, mat, output_shape=([2*x for x in image.shape]), \n",
    "               order=interp, mode='constant')\n",
    "    if cmap != 'RGB':\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    plt.grid()\n",
    "\n",
    "def reset_values(b):\n",
    "    for child in plot2.children:\n",
    "        if not hasattr(child, 'description'):\n",
    "            continue\n",
    "        elif child.description in ['tx', 'ty', 'θ', 'kx', 'ky']:\n",
    "            child.value = 0\n",
    "        elif child.description in ['s']:\n",
    "            child.value = 1.0\n",
    "\n",
    "reset_button = widgets.Button(description = \"Reset\")\n",
    "reset_button.on_click(reset_values)\n",
    "\n",
    "x2 = widgets.IntSlider(min=-200, max=200, step=9.8, orientation='vertical', description='$t_x$')\n",
    "y2 = widgets.IntSlider(min=-200, max=200, step=9.8, orientation='vertical', description='$t_y$')\n",
    "t2 = widgets.IntSlider(min=-180, max=180, step=18, orientation='vertical', description=r'$\\theta$')\n",
    "kx = widgets.FloatSlider(min=-2.0, max=2.0, value=0.0, orientation='vertical', description='$k_x$')\n",
    "ky = widgets.FloatSlider(min=-2.0, max=2.0, value=0.0, orientation='vertical', description='$k_y$')\n",
    "s = widgets.FloatSlider(min=0, max=2.0, value=1, orientation='vertical', description='$s$')\n",
    "interpolation = widgets.RadioButtons(\n",
    "    options=['Nearest Neighbor', 'Linear', 'Quadratic', 'Cubic'][::-1], value='Linear', description='interpolation')\n",
    "images = widgets.RadioButtons(options=['square', 'horse', 'coffee'])\n",
    "plot2 = interactive(f, tx=x2, ty=y2, θ=t2, s=s, kx=kx, ky=ky,interp=interpolation, image=images)\n",
    "layout = Layout(display='flex', flex_flow='row', justify_content='space-between')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b25906ffde470faeed96c20dce1edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), RadioButtons(description='image', options=('square', 'horse', 'coffee'), value='squar…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc121a8c7914250afdd508c0e39d185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=0, description='$t_x$', max=200, min=-200, orientation='vertical', step=9), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot2.update()\n",
    "display(HBox([plot2.children[-1], images]))\n",
    "display(HBox([*plot2.children[:-2], reset_button], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widget above shows for a particular parameterization (defined on the sliders) of the affine transformation, what the end effect will be on the image.\n",
    "You can choose between the square, an outline of a horse, and a photograph of a coffee cup.\n",
    "I encourage you to test out both the available parameters.\n",
    "You can also toggle different interpolation methods, to see how choice of sampling affects the output image (quadratic does not work due to a bug in the current version of scikit-image).\n",
    "\n",
    "### The Spatial Transformer Module: 2 pieces\n",
    "\n",
    "The Spatial Transformer consists of three pieces, a \"localization network\", a \"grid generator\", and a sampler\\*.\n",
    "\n",
    "![network configuration](img/STN_figure_2.png)\n",
    "\n",
    "##### the localization network\n",
    "\n",
    "The localization network is how the spatial transformation is decided for each input in the forward pass. \n",
    "It's simple in concept, it is a standard CNN, where the output units are the free parameters in the transformation model.\n",
    "For example, if we wanted an affine transformation, the localization network should have 6 output units.  For a similarity transform, 4 output units, etc.\n",
    "The rest is taken care of by backpropogation!\n",
    "The weight initialization should be small, making the model very close to an identity transform at first.\n",
    "\n",
    "##### the grid generator\n",
    "\n",
    "The transformation T is generated from the above network's parameters. Then we need to compute *which* points on the image we are sampling from after the transformation is applied, then place them into an output layer. The output layer may need not strictly be the same size as the input. In fact, the size of the output layer can be freely chosen.\n",
    "\n",
    "![grid generator](img/STN_figure_3.png)\n",
    "\n",
    "\n",
    "In the image above we can see the identity transformation contrasted with an affine transformation. The sampling grid generates the points which must be sampled onto. This is illustrated in the example above, where we see the quadrilateral of points on $U$ going directly into the next layer $V$. However, just like in the initial discussion of transformations, the points on the quadrilateral will not, in general, correspond directly to pixel coordinates in $U$. This is why we will need to *resample* the points.\n",
    "\n",
    "##### sampling\n",
    "\n",
    "The remaining issue is to recompute the value of the points we sample from on the image.  Because they are in between pixel coordinates, they require sampling. *In this context, sampling has nothing to do with statistical sampling, it is interpolation.*\n",
    "\n",
    "\n",
    "## By Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(y_train[..., np.newaxis])\n",
    "y_test = onehot_encoder.fit_transform(y_test[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc91c993e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[500][:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "We'll need to set up the pieces of the spatial transformer module, and then plug it into a standard CNN.\n",
    "\n",
    "First, we'll define the localizer.\n",
    "Remember that this is really the same as a small CNN.\n",
    "\n",
    "I'll also define a standard cnn for classification.\n",
    "This will help later for comparing STN results to regular networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localizer(input_layer, constraints):\n",
    "    if constraints is not None:\n",
    "        num_params = constraints.num_free_params\n",
    "    else:\n",
    "        num_params = 6\n",
    "    graph = keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(graph)\n",
    "    graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(graph)\n",
    "    graph = keras.layers.Flatten()(graph)\n",
    "\n",
    "    theta = keras.layers.Dense(units=num_params, activation='sigmoid')(graph)\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "def cnn_model_fn(input_layer, dense_units, num_classes):\n",
    "    graph = input_layer\n",
    "    for i in range(2):\n",
    "        graph = keras.layers.Conv2D(filters=32 * 2**i, kernel_size=(3, 3), activation='relu')(graph)\n",
    "        graph = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)(graph)\n",
    "    graph = keras.layers.Flatten()(graph)\n",
    "    graph = keras.layers.Dense(\n",
    "        units=dense_units, activation='sigmoid')(graph)\n",
    "    dense = keras.layers.Dense(num_classes)(graph)\n",
    "    logits = keras.layers.Activation('softmax', name='y')(dense)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def standard_cnn(**pm):\n",
    "    X = keras.layers.Input(\n",
    "        shape=pm['input_shape'], name='X')\n",
    "    output = cnn_model_fn(X, pm['dense_units'], pm['num_classes'])\n",
    "    model = keras.Model(inputs=X, outputs=output)\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a special module, using Deepmind Sonnet's tensorflow package.\n",
    "This module, which I'm calling the Spatial Transform Layer, is meant to take care of generating the grid points and the resampling.\n",
    "\n",
    "I've defined it to accept a tuple of inputs: the input image, and the transform parameters (the output layer of the localization network). The tuple input works more seamlessly with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformLayer(snt.AbstractModule):\n",
    "    \"\"\"\n",
    "    affine transform layer.\n",
    "    Constructor requires output_shape, with optional\n",
    "    constraints and boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_shape,\n",
    "                 constraints=None,\n",
    "                 name='stn_layer'):\n",
    "        \"\"\"\n",
    "        :param output_shape: shape of output layer (not including batch size)\n",
    "        :param constraints: AffineWarpConstraints object from dm-sonnet package\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        if constraints is None:\n",
    "            # default is full affine transform\n",
    "            constraints = snt.AffineWarpConstraints.no_constraints(num_dim=2)\n",
    "        self._constraints = constraints\n",
    "        self._output_shape = output_shape\n",
    "        self.__name__ = name\n",
    "\n",
    "    def _build(self, inputs):\n",
    "        \"\"\"\n",
    "        Layer requires a tuple of arguments: an input layer\n",
    "        and a set of transform parameters.\n",
    "        \"\"\"\n",
    "        U, theta = inputs\n",
    "        grid = snt.AffineGridWarper(\n",
    "            source_shape=U.get_shape().as_list()[1:-1],\n",
    "            output_shape=self._output_shape,\n",
    "            constraints=self._constraints)(theta)\n",
    "        V = tf.contrib.resampler.resampler(U, grid, name='resampler')\n",
    "\n",
    "        return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we have the tools to finalize the model.\n",
    "The format is basic, just to take inputs and transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our keras model\n",
    "def stn_model_fn(**pm):\n",
    "\n",
    "    # input layer\n",
    "    U = keras.layers.Input(\n",
    "        shape=pm['input_shape'], name='X')\n",
    "\n",
    "    # create localizationnetwork\n",
    "    theta = localizer(U, pm['constraints'])\n",
    "\n",
    "    grid_resampler = SpatialTransformLayer(output_shape=pm['output_shape'],\n",
    "                                           constraints=pm['constraints'])\n",
    "\n",
    "    # feed input U and parameters theta into the grid resampler\n",
    "    V = keras.layers.Lambda(\n",
    "        grid_resampler, output_shape=pm['output_shape'], name='V')([U, theta])\n",
    "\n",
    "    # now input V to a standard cnn\n",
    "    logits = cnn_model_fn(V, pm['dense_units'], pm['num_classes'])\n",
    "    \n",
    "    model = keras.Model(inputs=U, outputs=logits)\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# create some boundaries on the affine warp\n",
    "constraints = snt.AffineWarpConstraints([[None, 0, None],\n",
    "                                         [0, None, None]])\n",
    "\n",
    "model_parameters = {\n",
    "    'input_shape': (28, 28, 1),\n",
    "    'batch_size': 32,\n",
    "    'output_shape': (28, 28),\n",
    "    'constraints': constraints,\n",
    "    'dense_units': 128,\n",
    "    'num_classes': 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#normal_model = standard_cnn(**model_parameters)\n",
    "#normal_model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.5453 - acc: 0.8220 - val_loss: 0.2777 - val_acc: 0.8946\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.2791 - acc: 0.8954 - val_loss: 0.2499 - val_acc: 0.9031\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.2563 - acc: 0.9017 - val_loss: 0.2507 - val_acc: 0.9042\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.2384 - acc: 0.9065 - val_loss: 0.2369 - val_acc: 0.9082\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.2315 - acc: 0.9100 - val_loss: 0.2375 - val_acc: 0.9096\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2327 - acc: 0.9103 - val_loss: 0.2368 - val_acc: 0.9074\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.2291 - acc: 0.9103 - val_loss: 0.2386 - val_acc: 0.9078\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2282 - acc: 0.9121 - val_loss: 0.2330 - val_acc: 0.9149\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2262 - acc: 0.9137 - val_loss: 0.2442 - val_acc: 0.9093\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.2260 - acc: 0.9127 - val_loss: 0.2366 - val_acc: 0.9108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc91c943be0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = stn_model_fn(**model_parameters)\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
